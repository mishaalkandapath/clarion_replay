{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_TEST_GRIDS = {}\n",
    "ALL_TRAIN_GRIDS = {}   \n",
    "def read_h5py_string(dataset):\n",
    "    refs = dataset[()][0]  # Unpack the array of object references\n",
    "    strings = []\n",
    "    for ref in refs:\n",
    "        obj = dataset.file[ref]\n",
    "        string = obj[()].tobytes().decode(\"utf-16\")  # Decode the byte string\n",
    "        strings.append(string)\n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_ontopness(REQUIRED_FORM):\n",
    "    \"\"\"\n",
    "    Check if there is ontopness in pattern.\n",
    "    \"\"\"\n",
    "    ontopness = False\n",
    "    count_ontop = 0\n",
    "    ontop = 0\n",
    "    below = 0\n",
    "    \n",
    "    if REQUIRED_FORM.size > 0:  # Check if not empty\n",
    "        for i in range(1, REQUIRED_FORM.shape[0]):\n",
    "            row_current = REQUIRED_FORM.shape[0] - i\n",
    "            row_above = REQUIRED_FORM.shape[0] - (i + 1)\n",
    "            \n",
    "            if np.any(REQUIRED_FORM[row_current, :] - REQUIRED_FORM[row_above, :] != 0):\n",
    "                index = np.where((REQUIRED_FORM[row_current, :] - REQUIRED_FORM[row_above, :]) != 0)[0]\n",
    "                \n",
    "                if np.any((REQUIRED_FORM[row_current, index] * REQUIRED_FORM[row_above, index]) != 0):\n",
    "                    ontopness = True\n",
    "                    count_ontop = count_ontop + 1\n",
    "                    \n",
    "                    elements_form = np.unique(REQUIRED_FORM)\n",
    "                    elements_form = elements_form[elements_form != 0]\n",
    "                    \n",
    "                    row1 = np.where(REQUIRED_FORM == elements_form[0])[0]\n",
    "                    row2 = np.where(REQUIRED_FORM == elements_form[1])[0]\n",
    "                    \n",
    "                    if np.min(row1) < np.min(row2):\n",
    "                        ontop = elements_form[0]\n",
    "                        below = elements_form[1]\n",
    "                    elif np.min(row1) > np.min(row2):\n",
    "                        ontop = elements_form[1]\n",
    "                        below = elements_form[0]\n",
    "    \n",
    "    return ontopness, count_ontop, ontop, below\n",
    "\n",
    "def mk_besideness(REQUIRED_FORM):\n",
    "    \"\"\"\n",
    "    Check if there is besideness in pattern.\n",
    "    \"\"\"\n",
    "    besideness = False\n",
    "    count_beside = 0\n",
    "    left = 0\n",
    "    right = 0\n",
    "    \n",
    "    if REQUIRED_FORM.size > 0:  # Check if not empty\n",
    "        REQUIRED_FORM = REQUIRED_FORM.T\n",
    "        \n",
    "        for i in range(1, REQUIRED_FORM.shape[0]):\n",
    "            row_current = REQUIRED_FORM.shape[0] - i\n",
    "            row_above = REQUIRED_FORM.shape[0] - (i + 1)\n",
    "            \n",
    "            if np.any(REQUIRED_FORM[row_current, :] - REQUIRED_FORM[row_above, :] != 0):\n",
    "                index = np.where((REQUIRED_FORM[row_current, :] - REQUIRED_FORM[row_above, :]) != 0)[0]\n",
    "                \n",
    "                if np.any((REQUIRED_FORM[row_current, index] * REQUIRED_FORM[row_above, index]) != 0):\n",
    "                    besideness = True\n",
    "                    count_beside = count_beside + 1\n",
    "                    \n",
    "                    elements_form = np.unique(REQUIRED_FORM)\n",
    "                    elements_form = elements_form[elements_form != 0]\n",
    "                    \n",
    "                    row1 = np.where(REQUIRED_FORM == elements_form[0])[0]\n",
    "                    row2 = np.where(REQUIRED_FORM == elements_form[1])[0]\n",
    "                    \n",
    "                    if np.min(row1) < np.min(row2):\n",
    "                        left = elements_form[0]\n",
    "                        right = elements_form[1]\n",
    "                    elif np.min(row1) > np.min(row2):\n",
    "                        left = elements_form[1]\n",
    "                        right = elements_form[0]\n",
    "    \n",
    "    return besideness, count_beside, left, right\n",
    "\n",
    "def brick_connectedness(stim_grid):\n",
    "    bricks_conn_trial = [0, 0, 0] #Element connected to middle via besideness | middle Element | Element connected to middle via ontopness\n",
    "    bricks_rel_trial = [0, 0, 0, 0] # left element | ontop element | right element | below element\n",
    "\n",
    "    bricks = np.unique(stim_grid)[1:] # dont need 0 \n",
    "    \n",
    "    part1 = np.copy(stim_grid); part1[part1==bricks[0]] = 0;\n",
    "    part2 = np.copy(stim_grid); part2[part1==bricks[1]] = 0;\n",
    "    part3 = np.copy(stim_grid); part3[part1==bricks[2]] = 0;\n",
    "\n",
    "    bricks_order = np.array([[mk_ontopness(part3)[0]+mk_ontopness(part2)[0], mk_ontopness(part1)[0]+mk_ontopness(part3)[0], mk_ontopness(part1)[0]+mk_ontopness(part2)[0]], [mk_besideness(part3)[0]+mk_besideness(part2)[0], mk_besideness(part1)[0]+mk_besideness(part3)[0], mk_besideness(part1)[0]+mk_besideness(part2)[0]]])\n",
    "    bricks_order = ([np.where(~bricks_order[0, :] & bricks_order[1,:])[0],\n",
    "                     np.where(bricks_order[0,:] & bricks_order[1,:])[0],\n",
    "                       np.where(bricks_order[0,:] & ~bricks_order[1,:])[0]])\n",
    "    try:\n",
    "        bricks_conn_trial = bricks[bricks_order].T\n",
    "    except:\n",
    "        print(\"somethign happened here\")\n",
    "\n",
    "    if mk_ontopness(part1)[0]:\n",
    "        _, _, bricks_rel_trial[1], bricks_rel_trial[3] = mk_ontopness(part1)\n",
    "    elif mk_ontopness(part2)[0]:\n",
    "        _, _, bricks_rel_trial[1], bricks_rel_trial[3] = mk_ontopness(part2)\n",
    "    elif mk_ontopness(part3)[0]:\n",
    "        _, _, bricks_rel_trial[1], bricks_rel_trial[3] = mk_ontopness(part3)\n",
    "    \n",
    "    if mk_besideness(part1)[0]:\n",
    "        _, _, bricks_rel_trial[0], bricks_rel_trial[2] = mk_besideness(part1)\n",
    "    elif mk_besideness(part2)[0]:\n",
    "        _, _, bricks_rel_trial[0], bricks_rel_trial[2] = mk_besideness(part2)\n",
    "    elif mk_besideness(part3)[0]:\n",
    "        _, _, bricks_rel_trial[0], bricks_rel_trial[2] = mk_besideness(part3)\n",
    "\n",
    "    assert 0 not in bricks_conn_trial, \"0 in bricks_conn_trial\"\n",
    "    assert 0 not in bricks_rel_trial, \"0 in bricks_rel_trial\"\n",
    "    \n",
    "    return bricks_conn_trial.flatten(), bricks_rel_trial\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data_perparticipant(participant_num):\n",
    "    session_files = sorted(glob.glob(f\"/Users/mishaal/personalproj/clarion_replay/raw/Behav/s{participant_num}/T*.mat\"))\n",
    "    num_sessions = len(session_files)\n",
    "    meg_data = h5py.File(f\"/Users/mishaal/personalproj/clarion_replay/raw/data/s{participant_num}/Data_inference.mat\")\n",
    "    classifier_data = h5py.File(f\"/Users/mishaal/personalproj/clarion_replay/raw/data/s{participant_num}/Class_data.mat\")\n",
    "    \n",
    "    meg_signal_data = np.transpose(meg_data[\"data\"], (2, 1, 0))\n",
    "    meg_correct_dup = np.array(meg_data[\"correct_trials_all\"]).T\n",
    "\n",
    "\n",
    "    no_detect_grids = False\n",
    "\n",
    "    try:\n",
    "        bricks_conn_trial = np.array(meg_data[\"bricks_conn_trial\"]).T\n",
    "        bricks_rel_trial = np.array(meg_data[\"bricks_rel_trial\"]).T\n",
    "    except:\n",
    "        no_detect_grids = True\n",
    "\n",
    "    stim_labels = read_h5py_string(meg_data[\"stimlabel\"])# each unique presentation of a grid is given a label\n",
    "\n",
    "    assert num_sessions * 48 == len(stim_labels), f\"mismatch in trial numbers for participant {participant_num} {num_sessions * 48} {bricks_conn_trial.shape[0]}\"\n",
    "    #load the binomial classifiers \n",
    "    betas = np.array(classifier_data[\"betas_loc\"]).T\n",
    "    intercepts = np.array(classifier_data[\"intercepts_loc\"]).T\n",
    "\n",
    "    os.makedirs(f\"/Users/mishaal/personalproj/clarion_replay/processed/test_data/s{participant_num}\", exist_ok=True)\n",
    "    os.makedirs(f\"/Users/mishaal/personalproj/clarion_replay/processed/train_data/s{participant_num}\", exist_ok=True)\n",
    "\n",
    "    p_df = {\"PID\":[int(participant_num)]*len(stim_labels), \n",
    "            \"Session\": [], \"Trial\": [], \"Grid_Name\": [],\n",
    "            \"left_element\": [], \"ontop_element\": [], \"right_element\": [], \"below_element\": [],\n",
    "            \"besideness\": [], \"middle\": [], \"ontopness\": [], \n",
    "              \"Q_Brick_Middle\": [], \"Q_Brick_Left\": [], \"Q_Relation\": [], \"True Relation\": [], \"Correct\": [], \"RT\":[]}\n",
    "\n",
    "    absolute_trial_index = 0\n",
    "    for idx, filename in enumerate(session_files):\n",
    "        all_data = loadmat(filename)\n",
    "        behav_data = all_data[\"res\"][0, 0][\"behav\"][0,0]\n",
    "        stimulus_grids = behav_data[\"SOLUTIONS_BUILT\"]\n",
    "        correctness = behav_data[\"correct\"]\n",
    "        rts = behav_data[\"rt\"]\n",
    "        q_stimuli = behav_data[\"stim_catch\"]\n",
    "        query_relation = behav_data[\"question_catch\"] # a brick is presented in the middle and another on the top left corner. the relation of the top left brick to the middle brick is asked. This is the identity of the brick in the top left corner\n",
    "        true_relation = behav_data[\"relation_catch\"] # the relation in question\n",
    "\n",
    "        # save experiment data\n",
    "        n_trials = correctness.shape[1]\n",
    "        p_df[\"Session\"].extend([idx+1]*n_trials)\n",
    "        p_df[\"Trial\"].extend(list(range(1, n_trials+1)))\n",
    "        # turns out labeling not unique?\n",
    "        stim_label = stim_labels[absolute_trial_index: absolute_trial_index + n_trials]\n",
    "        # p_df[\"Grid_Name\"].extend(stim_label)\n",
    "\n",
    "\n",
    "        if not no_detect_grids:\n",
    "            p_df[\"left_element\"].extend([bricks_rel_trial[i, 0] for i in range(absolute_trial_index, absolute_trial_index + n_trials)])\n",
    "            p_df[\"ontop_element\"].extend([bricks_rel_trial[i, 1] for i in range(absolute_trial_index, absolute_trial_index + n_trials)])\n",
    "            p_df[\"right_element\"].extend([bricks_rel_trial[i, 2] for i in range(absolute_trial_index, absolute_trial_index + n_trials)])\n",
    "            p_df[\"below_element\"].extend([bricks_rel_trial[i, 3] for i in range(absolute_trial_index, absolute_trial_index + n_trials)])\n",
    "\n",
    "            p_df[\"besideness\"].extend([bricks_conn_trial[i, 0] for i in range(absolute_trial_index, absolute_trial_index + n_trials)])\n",
    "            p_df[\"middle\"].extend([bricks_conn_trial[i, 1] for i in range(absolute_trial_index, absolute_trial_index + n_trials)])\n",
    "            p_df[\"ontopness\"].extend([bricks_conn_trial[i, 2] for i in range(absolute_trial_index, absolute_trial_index + n_trials)])\n",
    "\n",
    "        p_df[\"Q_Brick_Left\"].extend(q_stimuli[:, 0].flatten())\n",
    "        p_df[\"Q_Brick_Middle\"].extend(q_stimuli[:, 1].flatten())\n",
    "        p_df[\"Q_Relation\"].extend(query_relation.flatten())\n",
    "        p_df[\"True Relation\"].extend(true_relation.flatten())\n",
    "\n",
    "        p_df[\"Correct\"].extend(correctness.flatten())\n",
    "        p_df[\"RT\"].extend(rts.flatten())\n",
    "\n",
    "        for j in range(n_trials): # add the grid\n",
    "            t = 0\n",
    "            for grid in ALL_TEST_GRIDS:\n",
    "                if np.all(ALL_TEST_GRIDS[grid] == stimulus_grids[:, :, j]):\n",
    "                    t = 1\n",
    "                    p_df[\"Grid_Name\"].append(grid)\n",
    "                    break\n",
    "            if t == 0: \n",
    "                ALL_TEST_GRIDS[f\"GRID{len(ALL_TEST_GRIDS)}\"] = stimulus_grids[:, :, j]\n",
    "                p_df[\"Grid_Name\"].append(f\"GRID{len(ALL_TEST_GRIDS)-1}\")\n",
    "\n",
    "            if no_detect_grids:\n",
    "                bricks_conn_trial, bricks_rel_trial = brick_connectedness(stimulus_grids[:, :, j])\n",
    "\n",
    "                p_df[\"left_element\"].append(bricks_rel_trial[0])\n",
    "                p_df[\"ontop_element\"].append(bricks_rel_trial[1])\n",
    "                p_df[\"right_element\"].append(bricks_rel_trial[2])\n",
    "                p_df[\"below_element\"].append(bricks_rel_trial[3])\n",
    "\n",
    "                p_df[\"besideness\"].append(bricks_conn_trial[0])\n",
    "                p_df[\"middle\"].append(bricks_conn_trial[1])\n",
    "                p_df[\"ontopness\"].append(bricks_conn_trial[2])\n",
    "            \n",
    "            # else: # sanity checking\n",
    "            #     bricks_conn_trial_, bricks_rel_trial_ = brick_connectedness(stimulus_grids[:, :, j])\n",
    "            #     assert np.all(bricks_conn_trial[absolute_trial_index + j] == bricks_conn_trial_), f\"brick_conn_trial mismatch for participant {participant_num} session {idx+1} trial {j+1}\"\n",
    "            #     assert np.all(bricks_rel_trial[absolute_trial_index + j] == bricks_rel_trial_), f\"brick_rel_trial mismatch for participant {participant_num} session {idx+1} trial {j+1}\"\n",
    "\n",
    "        # if indeed grids were uniquely named:\n",
    "        # for j in range(n_trials):\n",
    "        #     grid_name = p_df[\"Grid_Name\"][absolute_trial_index + j]\n",
    "        #     if grid_name not in ALL_TEST_GRIDS:\n",
    "        #         ALL_TEST_GRIDS[grid_name] = [stimulus_grids[:, :, j], bricks_rel_trial[absolute_trial_index + j], bricks_conn_trial[absolute_trial_index + j]]\n",
    "        #     else:\n",
    "        #         assert np.all(ALL_TEST_GRIDS[grid_name] == stimulus_grids[:, :, j]), f\"grid mismatch for participant {participant_num} session {idx+1} trial {j+1}\"\n",
    "\n",
    "        absolute_trial_index += n_trials\n",
    "\n",
    "    #Save MEG data\n",
    "    np.save(f\"/Users/mishaal/personalproj/clarion_replay/processed/test_data/s{participant_num}/meg_data.npy\", meg_signal_data)\n",
    "    #save classifier data\n",
    "    np.save(f\"/Users/mishaal/personalproj/clarion_replay/processed/test_data/s{participant_num}/classifier_coeffs.npy\", betas)    \n",
    "    np.save(f\"/Users/mishaal/personalproj/clarion_replay/processed/test_data/s{participant_num}/classifier_intercepts.npy\", intercepts)\n",
    "    assert np.all(meg_correct_dup.flatten() == np.array(p_df[\"Correct\"])), f\"correctness mismatch for participant {participant_num}\"  \n",
    "\n",
    "    p_df = pd.DataFrame(p_df)\n",
    "    p_df.to_csv(f\"/Users/mishaal/personalproj/clarion_replay/processed/test_data/s{participant_num}/test_data.csv\")\n",
    "\n",
    "    return p_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0a691b8d5d4b9a81e7591ec22113d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "participants = glob.glob(\"/Users/mishaal/personalproj/clarion_replay/raw/Behav/s*\")\n",
    "participants = sorted([(p.split(\"/\")[-1][1:]) for p in participants])\n",
    "dfs = []\n",
    "for p in tqdm(participants):\n",
    "    if \"18\" in p: continue\n",
    "    p_df = load_test_data_perparticipant(p)\n",
    "    dfs.append(p_df)\n",
    "\n",
    "# concatenate dataframes\n",
    "pd.concat(dfs).to_csv(\"/Users/mishaal/personalproj/clarion_replay/processed/test_data/all_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_construction_data_participant(participant_num):\n",
    "    session_files = sorted(glob.glob(f\"/Users/mishaal/personalproj/clarion_replay/raw/Behav/Training_MEG/s{participant_num}/T*.mat\"))\n",
    "    num_sessions = len(session_files)\n",
    "\n",
    "    p_df = {\"PID\":[], \"Session\": [], \"Trial\": [], \"Grid_Name\": []}\n",
    "\n",
    "    for idx, filename in enumerate(session_files):\n",
    "        all_data = loadmat(filename)\n",
    "        behav_data = all_data[\"res_train\"][0, 0]\n",
    "        stimulus_grids = behav_data[\"INFO_FORM\"]\n",
    "        correctness = behav_data[\"correct\"]\n",
    "\n",
    "        n_trials = correctness.shape[1]\n",
    "        p_df[\"Session\"].extend([idx+1]*n_trials)\n",
    "        p_df[\"Trial\"].extend(list(range(1, n_trials+1)))\n",
    "        p_df[\"PID\"].extend([int(participant_num)]*n_trials)\n",
    "\n",
    "        for j in range(n_trials):\n",
    "            for grid in ALL_TRAIN_GRIDS:\n",
    "                if np.all(ALL_TRAIN_GRIDS[grid] == stimulus_grids[:, :, j]):\n",
    "                    p_df[\"Grid_Name\"].append(grid)\n",
    "                    break\n",
    "            else:\n",
    "                ALL_TRAIN_GRIDS[f\"GRID{len(ALL_TRAIN_GRIDS)}\"] = stimulus_grids[:, :, j]\n",
    "                p_df[\"Grid_Name\"].append(f\"GRID{len(ALL_TRAIN_GRIDS)-1}\")\n",
    "\n",
    "    #write the csv\n",
    "    p_df = pd.DataFrame(p_df)\n",
    "    p_df.to_csv(f\"/Users/mishaal/personalproj/clarion_replay/processed/train_data/s{participant_num}/train_data_constr.csv\")\n",
    "    return p_df\n",
    "\n",
    "\n",
    "def load_train_rel_data_participant(participant_num):\n",
    "    session_files = sorted(glob.glob(f\"/Users/mishaal/personalproj/clarion_replay/raw/Behav/Training_MEG/s{participant_num}/D*.mat\"))\n",
    "    num_sessions = len(session_files)\n",
    "\n",
    "    p_df = {\"PID\":[], \"Session\": [], \"Trial\": [], \"Grid_Name\": []}\n",
    "    \n",
    "    for idx, filename in enumerate(session_files):\n",
    "        all_data = loadmat(filename)\n",
    "        behav_data = all_data[\"res_train\"][0, 0]\n",
    "        stimulus_grids = behav_data[\"SOLUTIONS\"]\n",
    "\n",
    "        n_trials = stimulus_grids.shape[2]\n",
    "        p_df[\"Session\"].extend([idx+1]*n_trials)\n",
    "        p_df[\"Trial\"].extend(list(range(1, n_trials+1)))\n",
    "        p_df[\"PID\"].extend([int(participant_num)]*n_trials)\n",
    "\n",
    "        for j in range(n_trials):\n",
    "            for grid in ALL_TRAIN_GRIDS:\n",
    "                if np.all(ALL_TRAIN_GRIDS[grid] == stimulus_grids[:, :, j]):\n",
    "                    p_df[\"Grid_Name\"].append(grid)\n",
    "                    break\n",
    "            else:\n",
    "                ALL_TRAIN_GRIDS[f\"GRID{len(ALL_TRAIN_GRIDS)}\"] = stimulus_grids[:, :, j]\n",
    "                p_df[\"Grid_Name\"].append(f\"GRID{len(ALL_TRAIN_GRIDS)-1}\")\n",
    "\n",
    "    #write the csv\n",
    "    p_df = pd.DataFrame(p_df)\n",
    "    p_df.to_csv(f\"/Users/mishaal/personalproj/clarion_replay/processed/train_data/s{participant_num}/train_data_rel.csv\")\n",
    "    return p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f976283fc64a038fbafdfbaeb24139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "participants = glob.glob(\"/Users/mishaal/personalproj/clarion_replay/raw/Behav/s*\")\n",
    "participants = sorted([(p.split(\"/\")[-1][1:]) for p in participants])\n",
    "dfs = []\n",
    "for p in tqdm(participants):\n",
    "    if \"18\" in p: continue\n",
    "    # p_df = load_train_construction_data_participant(p)\n",
    "    p_df = load_train_rel_data_participant(p)\n",
    "    dfs.append(p_df)\n",
    "\n",
    "# concatenate dataframes\n",
    "# pd.concat(dfs).to_csv(\"/Users/mishaal/personalproj/clarion_replay/processed/train_data/all_train_cons_data.csv\")\n",
    "pd.concat(dfs).to_csv(\"/Users/mishaal/personalproj/clarion_replay/processed/train_data/all_train_rel_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tapnseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
